{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0Cku-XWbabE"
      },
      "source": [
        "## RISC Summer Internship Demonstration June 2024\n",
        "\n",
        "This CoLab notebook is designed to give a brief demonstration into some of the readily accessible capabilities of computer vision applications available to both experts and non-experts. An incredible amount of resources are available on GitHub, YouTube, and other common sites.\n",
        "\n",
        "Please ask questions and explore on your own this exciting area. The two most popular deep learning frameworks are PyTorch (https://pytorch.org/) and Keras/TensorFlow (https://tensorflow.org/). If you want to see what you can do directly in these deep learning frameworks, you can start here: https://pytorch.org/examples/ or here: https://keras.io/examples/.\n",
        "\n",
        "We will start with the popular \"You Only Look Once\" YOLO architecture implemented by Ultralytics in PyTorch (https://github.com/ultralytics/ultralytics). This framework has evolved from an object detection tool (detecting objects and their bounds within an image) into a tool that does object detection, semantic segmentation of an image (segmenting of different object), pose detection, and tracking. This tool is provided as initially trained on the COCO dataset with facilities to provide custom training.\n",
        "\n",
        "---\n",
        "\n",
        "Alan McMillan, PhD  \n",
        "University of Wisconsin  \n",
        "abmcmillan@wisc.edu  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaCvSr8ATOg-"
      },
      "outputs": [],
      "source": [
        "# first, install YOLO\n",
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFsSDBQ7tGVA"
      },
      "source": [
        "# Object Detection\n",
        "Let's do an example of object detection in an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU8xDlP5TTI3"
      },
      "outputs": [],
      "source": [
        "# lets start with a simple example for object detection\n",
        "# we can give the YOLO tool a local file or a web address\n",
        "# and here we are calling the command line YOLO tool (we can do this in this within\n",
        "# this notebook as below or from a command prompt if you are running Jupyter notebook\n",
        "# on a local machine)\n",
        "! yolo mode=predict task=detect model=yolov8n.pt save_txt save_conf exist_ok name='example1' source='https://thecityfix.com/wp-content/uploads/2013/11/Sao-Paulo-pedestrian-cross-walk-Fred-Inklaar-640x480.jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `example1` folder there is a sub-folder `labels` which contains a text file which stores for each object detected, its class, a confidence in prediction, and the location of a bounding box.\n",
        "\n",
        "Here are the first five lines of this file:\n",
        "```\n",
        "0 0.405748 0.545688 0.109717 0.398905 0.895639\n",
        "0 0.28348 0.556874 0.0933392 0.356039 0.881168\n",
        "0 0.101321 0.650908 0.133359 0.506478 0.86\n",
        "0 0.0277055 0.613384 0.0554109 0.434047 0.796258\n",
        "3 0.792999 0.609702 0.168179 0.257695 0.745829\n",
        "```\n",
        "\n",
        "The class 0 corresponds to `person` and class 3 corresponds to `motorcycle`. You can find more information about other labels in the COCO dataset here: https://docs.ultralytics.com/datasets/detect/coco/#dataset-yaml\n",
        "\n",
        "The next number is the confidence in prediction provided by YOLO.\n",
        "\n",
        "The remaining four numbers are the coordinates of the bounding box in the following order: center location in x, center location in y, width, height.\n",
        "\n",
        "We will discuss this output in more detail in just a little bit.\n"
      ],
      "metadata": {
        "id": "5HxSt1hKkVY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fae8jBneTmO5"
      },
      "outputs": [],
      "source": [
        "# let's display the image here in our Notebook\n",
        "from pathlib import Path\n",
        "from IPython.display import Image, display\n",
        "# find the output file\n",
        "image_file = list(Path('./runs/detect/example1').glob('Sao-Paulo*.jpg'))[0]\n",
        "# display it\n",
        "display(Image(filename=image_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JNQ_6Wbjvv6"
      },
      "outputs": [],
      "source": [
        "# we can also do this directly from within Python\n",
        "# (https://docs.ultralytics.com/models/yolov8/#supported-modes)\n",
        "from ultralytics import YOLO\n",
        "# Load the model\n",
        "model = YOLO('yolov8m.pt')  # load a pretrained model\n",
        "\n",
        "# Perform inference\n",
        "results = model('https://thecityfix.com/wp-content/uploads/2013/11/Sao-Paulo-pedestrian-cross-walk-Fred-Inklaar-640x480.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXX_Xj70n2Sm"
      },
      "outputs": [],
      "source": [
        "from matplotlib.figure import Bbox\n",
        "# we can now do a simple plot from the detected bounding boxes, you could also do\n",
        "# something more complicated here or leverage the existing code that is used to\n",
        "# write the output file as above\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "fig, ax = plt.subplots(1)\n",
        "img = cv2.cvtColor(results[0].orig_img, cv2.COLOR_BGR2RGB)\n",
        "ax.imshow( img )\n",
        "\n",
        "# Iterate over detections and draw bounding boxes\n",
        "for i,box in enumerate(results[0].boxes.cpu()):\n",
        "    pltbox = plt.Rectangle((box.xyxy[0][0], box.xyxy[0][1]), box.xyxy[0][2]-box.xyxy[0][0], box.xyxy[0][3]-box.xyxy[0][1], fill=False, color='red')\n",
        "    ax.add_patch(pltbox)\n",
        "    pltlabel = f'{results[0].names[box.cls.item()]}: {box.conf.item():.2f}'\n",
        "    plt.text(box.xyxy[0][0], box.xyxy[0][1], pltlabel, color='white' )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGPwjQljrr3-"
      },
      "source": [
        "So let's talk a little bit more about the data format of the object detection system. For each object present in an image, there's a corresponding line in the text file. Each line represents one bounding box. The format of each line is as follows:\n",
        "\n",
        "```\n",
        "<object-class> <x_center> <y_center> <width> <height>\n",
        "```\n",
        "```<object-class>```: The class of the object. This is an integer where each integer represents a different type of identifiable object (for example, \"0\" might correspond to a car, \"1\" might correspond to a person, etc.). The class numbers are zero-indexed if there are multiple classes. In YOLOv8, the output class types can be seen in ```results[i].names```.\n",
        "\n",
        "```<x_center> <y_center> <width> <height>```: These values are relative to the width and the height of the image. They all are floating point values ranging from 0 to 1.\n",
        "\n",
        "```<x_center>, <y_center>```: These values represent the center of the bounding box containing the object, relative to the dimensions of the image.\n",
        "\n",
        "```<width>, <height>```: These values represent the width and the height of the bounding box, also relative to the dimensions of the image.\n",
        "\n",
        "In this format, the top left of the image is (0,0), and the bottom right of the image is (1,1). Note that during prediction, the results have the output boxes in the pixel units of the input image.\n",
        "\n",
        "**We will need this if we want to add our own data to the model, which we will discuss later.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6fstJiytPfg"
      },
      "source": [
        "# Semantic Segmentation\n",
        "Let's explore the use of this framework to do segmentation instead of object detection. In this way, instead of a bounding box, we get a label for each pixel for its respective classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asTEPyjootoS"
      },
      "outputs": [],
      "source": [
        "# let's use the command line YOLO tool (we can do this in this within this notebook as below\n",
        "# or from a command prompt if you are running Jupyter notebook on a local machine)\n",
        "! yolo mode=predict task=segment model=yolov8m-seg.pt save_txt save_conf exist_ok name='example2' source='https://thecityfix.com/wp-content/uploads/2013/11/Sao-Paulo-pedestrian-cross-walk-Fred-Inklaar-640x480.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vczMeeeguYMG"
      },
      "outputs": [],
      "source": [
        "# find the output file and display it\n",
        "image_file = list(Path('./runs/segment/example2').glob('Sao-Paulo*.jpg'))[0]\n",
        "display(Image(filename=image_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rcUJlPevEXp"
      },
      "source": [
        "Ok that looks cool, but it has hard to see what we have, let's change the option on YOLO to remove the boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKNsJwWMvQRD"
      },
      "outputs": [],
      "source": [
        "! yolo mode=predict task=segment model=yolov8n-seg.pt show_labels=false show_conf=false show_boxes=false save_txt save_conf exist_ok name='example3' source='https://thecityfix.com/wp-content/uploads/2013/11/Sao-Paulo-pedestrian-cross-walk-Fred-Inklaar-640x480.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRPcl9H9kcxe"
      },
      "outputs": [],
      "source": [
        "# find the output file and display it\n",
        "image_file = list(Path('./runs/segment/example3').glob('Sao-Paulo*.jpg'))[0]\n",
        "display(Image(filename=image_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZdUHqCWzcMD"
      },
      "source": [
        "Here we can see that the image missed a few objects. We are using the smallest model `yolov8n-seg` (https://docs.ultralytics.com/models/yolov8/#performance-metrics). Maybe if we try with a larger model, we will have a better result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lfx7u9a8z6By"
      },
      "outputs": [],
      "source": [
        "# As before, but now use the yolov8l-seg model\n",
        "! yolo mode=predict task=segment model=yolov8l-seg.pt show_labels=false show_conf=false show_boxes=false save_txt save_conf exist_ok name='example4' source='https://thecityfix.com/wp-content/uploads/2013/11/Sao-Paulo-pedestrian-cross-walk-Fred-Inklaar-640x480.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ETI6fZb0PGL"
      },
      "outputs": [],
      "source": [
        "# find the output file and display it\n",
        "image_file = list(Path('./runs/segment/example4').glob('Sao-Paulo*.jpg'))[0]\n",
        "display(Image(filename=image_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSUlmzv80b9C"
      },
      "source": [
        "That looks like it improved the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRonV7Z_0gYM"
      },
      "source": [
        "# Pose Estimation\n",
        "YOLO also has the ability to estimate poses in datasets. You can read more about it here: https://docs.ultralytics.com/tasks/pose/. But let's try it out on a new image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntw4cEJbV5rv"
      },
      "outputs": [],
      "source": [
        "# test on an image with YOLOv8n\n",
        "!yolo mode=predict task=pose model=yolov8l-pose.pt name='example5' source='https://fbschedules.com/wp-content/uploads/2017/01/maryland-virginia-football-768x512.jpg'\n",
        "# find the output file and display it\n",
        "image_file = list(Path('./runs/pose/example5').glob('maryland-virginia*'))[0]\n",
        "display(Image(filename=image_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgnMhXap4Gdm"
      },
      "source": [
        "# Using your own data\n",
        "The default models here were trained on the COCO dataset. This has 80 categories of common objects, but it does not encompass everything that we might see. Therefore, adapting this model to new data is particularly useful and powerful. Let's explore a quick example doign this. Here I downloaded a public dataset of thermal images of humans and dogs from Roboflow (https://public.roboflow.com/object-detection/thermal-dogs-and-people).\n",
        "\n",
        "The general process to do this can be found here: https://docs.ultralytics.com/modes/train/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6Ar0UCK5vhZ"
      },
      "outputs": [],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR08cYid66kv"
      },
      "source": [
        "My dataset is in a folder `/content/drive/ThermalDogsAndPeople`, yours could be in a different folder.\n",
        "\n",
        "Let's look at the data first. There is a configuration file `data.yaml` that describes our data. Here we have two classes: 'dog' and 'person'.\n",
        "\n",
        "Here is the conntent of `data.yaml`:\n",
        "```\n",
        "train: ../train/images\n",
        "val: ../valid/images\n",
        "test: ../test/images\n",
        "\n",
        "nc: 2\n",
        "names: ['dog', 'person']\n",
        "\n",
        "roboflow:\n",
        "  workspace: joseph-nelson\n",
        "  project: thermal-dogs-and-people\n",
        "  version: 1\n",
        "  license: Public Domain\n",
        "  url: https://universe.roboflow.com/joseph-nelson/thermal-dogs-and-people/dataset/1\n",
        "```\n",
        "\n",
        "The `train`, `val`, and `test` entries correspond to the folders where our data lives. Each folder includes subfolders `labels` and `images` where each image files has a corresponding matching label file with the same name, but ending in `.txt`\n",
        "\n",
        "The `nc` entry is the number of classes in this dataset. We have two: `dog` and `person`.\n",
        "\n",
        "The `roboflow` entry contains metadata about the dataset.\n",
        "\n",
        "Before we try something fancy, lets try to see how a default YOLO model performs on this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVJ08ZgE7_Om"
      },
      "outputs": [],
      "source": [
        "! yolo mode=predict task=detect model=yolov8n.pt save_txt save_conf exist_ok name='example6' source='/content/drive/MyDrive/ThermalDogsAndPeople/test/images/IMG_0006 5_jpg.rf.cd46e6a862d6ffb7fce6795067ce7cc7.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K02WOps66uRT"
      },
      "outputs": [],
      "source": [
        "# find the output file and display it\n",
        "image_file = list(Path('./runs/detect/example6').glob('IMG*.jpg'))[0]\n",
        "display(Image(filename=image_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkWCP5AR9Uoh"
      },
      "source": [
        "Oops, that didn't work so well. Let's see if we can improve the result be refining our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOa6i9Ra9YvF"
      },
      "outputs": [],
      "source": [
        "# let's try to train\n",
        "! yolo task=detect mode=train model=yolov8n.pt epochs=10 imgsz=640 data=/content/drive/MyDrive/ThermalDogsAndPeople/data.yaml\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fus54o58_tzq"
      },
      "source": [
        "This is a very short amount of training, and is not realistic in practice. Our model will perform better if we trained it for longer. However, let's now try the same image with our trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54oA01eF_xLj"
      },
      "outputs": [],
      "source": [
        "! yolo mode=predict task=detect model=runs/detect/train/weights/best.pt save_txt save_conf exist_ok name='example7' source='/content/drive/MyDrive/ThermalDogsAndPeople/test/images/IMG_0006 5_jpg.rf.cd46e6a862d6ffb7fce6795067ce7cc7.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW7iaKrj_-Bt"
      },
      "outputs": [],
      "source": [
        "# find the output file and display it\n",
        "image_file = list(Path('./runs/detect/example7').glob('IMG*.jpg'))[0]\n",
        "display(Image(filename=image_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct_tY4-qAVaa"
      },
      "source": [
        "With just a little fine tuning, we were able to get better results. With the right amount of data, you will be able to apply these algorithms to many new problems. The biggest challenge you will face will likely be obtaining, curating, and labeling data. This will be specific to the domain of your problem.\n",
        "\n",
        "## Getting Labels for Your Data\n",
        "\n",
        "There exist several tools to help you with labeling your data. One is the website Roboflow (https://roboflow.com) and there exist tools you can run localling, like Yolo_Label (https://github.com/developer0hye/Yolo_Label) and Label Studio (https://github.com/heartexlabs/label-studio). There are invariably many other tools that exist.\n",
        "\n",
        "## Segment Anything Model\n",
        "\n",
        "A promising new approach is to utilize a foundation model for image segmentation such as Meta's Segment Anything (https://segment-anything.com/). This model can identify objects within images and be used to define segmented regions and/or bounding boxes, and this can be used to automate a human-in-the-loop active learning pipeline to label images. Let's try it from YOLO (https://docs.ultralytics.com/models/sam/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoqwY-SnJNpl"
      },
      "outputs": [],
      "source": [
        "# try the Segment Anything Model\n",
        "from ultralytics import SAM\n",
        "model = SAM('sam_l.pt')\n",
        "model.info()  # display model information\n",
        "sam_result = model.predict(name='example8', save=True, show_labels=False, show_boxes=False, source='https://fbschedules.com/wp-content/uploads/2017/01/maryland-virginia-football-768x512.jpg')  # predict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now find the output file and display it\n",
        "image_file = list(Path('./runs/segment/example8').glob('*.jpg'))[0]\n",
        "display(Image(filename=image_file))"
      ],
      "metadata": {
        "id": "K6DvF4pRfYZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLM_UFzZM_9s"
      },
      "source": [
        "The interesting and exciting thing about the Segment Anything foundation model is that it is not trained to detect specific objects of any class, but rather any distinct object in an image. This means that it can be used in a manner to potentially automatically label images. In particular this tool could be very useful to take an object detection dataset and make it into a segmentation dataset. Additionally, it is much less work for a human to perform object detection compared to segmentation. This automatic annotation approach approach is rapidly being developed in the field. For example, see this: https://docs.ultralytics.com/models/sam/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision-Language Models\n",
        "\n",
        "Another interesting approach is the use of multi-modal models. While we didn't have time to go deeply into this in our lecture, these models allow the use of open vocabulary for object classes. Using a strategy based on the concept of Contrastive Language-Image Pretraining (CLIP) proposed by OpenAI in 2021 (https://openai.com/index/clip/), we do not necessarily need to know the explicit class of our object, which can reduce the need for training bespoke models. Note that this concept is used in all modern multi-modal large language models like GPT-4o, Claude, and Gemini.\n",
        "\n",
        "Let's try out a smaller implementation called YOLO-World: https://docs.ultralytics.com/models/yolo-world/"
      ],
      "metadata": {
        "id": "s7oe724geGG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLOWorld\n",
        "\n",
        "# Initialize a YOLO-World model\n",
        "model = YOLOWorld(\"yolov8s-world.pt\")  # or select yolov8m/l-world.pt for different sizes\n",
        "\n",
        "# Execute inference with the YOLOv8s-world model on the specified image\n",
        "results = model.predict(name='example9', save=True, save_txt=True, source=\"https://thecityfix.com/wp-content/uploads/2013/11/Sao-Paulo-pedestrian-cross-walk-Fred-Inklaar-640x480.jpg\")"
      ],
      "metadata": {
        "id": "wB390WkdjfTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the output file\n",
        "image_file = list(Path('./runs/detect/example9').glob('Sao-Paulo*.jpg'))[0]\n",
        "# display it\n",
        "display(Image(filename=image_file))"
      ],
      "metadata": {
        "id": "e4Z-XOFxmfLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This also saves a file with all of the detected labels.\n",
        "\n",
        "We can also specify which classes we want to find. Let's try it:"
      ],
      "metadata": {
        "id": "lVtfyfY5pGGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.set_classes([\"shoe\", \"belt\",\"handbag\"])\n",
        "\n",
        "# Execute prediction for specified categories on an image\n",
        "results = model.predict(name='example10', save=True, save_txt=True, source=\"https://thecityfix.com/wp-content/uploads/2013/11/Sao-Paulo-pedestrian-cross-walk-Fred-Inklaar-640x480.jpg\")"
      ],
      "metadata": {
        "id": "aP0GV2eJpSpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the output file\n",
        "image_file = list(Path('./runs/detect/example10').glob('Sao-Paulo*.jpg'))[0]\n",
        "# display it\n",
        "display(Image(filename=image_file))"
      ],
      "metadata": {
        "id": "2F54ITUksnPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This brings a large degree of capability to object detection without the need for customization of an entire model. It could also be used to help rapidly create a dataset for conventional supervised training."
      ],
      "metadata": {
        "id": "RqxiEL_WqF60"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBK4axKGONzk"
      },
      "source": [
        "# Finish Line\n",
        "This completes this exercise and survey of computer vision for object detection and segmentation. Computer vision is a fast moving field and there are many great tools that are available. This will hopefully get you started. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz0PlqfKOXOy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}